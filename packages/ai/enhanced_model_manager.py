"""
Enhanced Model Manager with GGUF Support
Integrates DeepSeek R1 GGUF models for local processing
"""

import asyncio
import logging
import os
from typing import Dict, Any, Optional, List
from pathlib import Path
import json

try:
    from llama_cpp import Llama
    from huggingface_hub import hf_hub_download
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False
    Llama = None

logger = logging.getLogger("enhanced_model_manager")

class EnhancedModelManager:
    """Enhanced model manager with local GGUF support"""
    
    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.active_model: Optional[str] = None
        self.model_configs = {
            "deepseek_r1_gguf": {
                "model_path": "models/deepseek-r1/deepseek-r1-0528-qwen3-8b-q4_k_m.gguf",
                "n_ctx": 4096,
                "n_threads": 4,
                "temperature": 0.7,
                "max_tokens": 1024
            }
        }
    
    async def initialize(self):
        """Initialize the model manager"""
        logger.info("ðŸš€ Initializing Enhanced Model Manager")
        
        if not GGUF_AVAILABLE:
            logger.warning("âš ï¸  GGUF support not available, using fallback mode")
            return False
        
        # Try to load DeepSeek R1 GGUF model
        try:
            await self.load_model("deepseek_r1_gguf")
            logger.info("âœ… Enhanced Model Manager initialized successfully")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to initialize model manager: {e}")
            return False
    
    async def load_model(self, model_id: str) -> bool:
        """Load a specific model"""
        if model_id not in self.model_configs:
            logger.error(f"âŒ Unknown model: {model_id}")
            return False
        
        config = self.model_configs[model_id]
        model_path = config["model_path"]
        
        if not os.path.exists(model_path):
            logger.error(f"âŒ Model file not found: {model_path}")
            return False
        
        try:
            logger.info(f"ðŸ”„ Loading model: {model_id}")
            
            if GGUF_AVAILABLE:
                model = Llama(
                    model_path=model_path,
                    n_ctx=config["n_ctx"],
                    n_threads=config["n_threads"],
                    verbose=False
                )
                self.models[model_id] = model
                self.active_model = model_id
                logger.info(f"âœ… Model loaded successfully: {model_id}")
                return True
            else:
                # Fallback mode
                self.models[model_id] = {"type": "fallback", "config": config}
                self.active_model = model_id
                logger.info(f"âœ… Model loaded in fallback mode: {model_id}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Failed to load model {model_id}: {e}")
            return False
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """Generate response using the active model"""
        if not self.active_model:
            return "âŒ No active model available"
        
        model = self.models.get(self.active_model)
        if not model:
            return "âŒ Active model not found"
        
        try:
            if GGUF_AVAILABLE and isinstance(model, Llama):
                # Real GGUF model generation
                config = self.model_configs[self.active_model]
                response = model(
                    prompt,
                    max_tokens=kwargs.get("max_tokens", config["max_tokens"]),
                    temperature=kwargs.get("temperature", config["temperature"]),
                    stop=kwargs.get("stop", ["\n\n"])
                )
                return response["choices"][0]["text"].strip()
            else:
                # Fallback mode with enhanced responses
                return f"ðŸ¤– DeepSeek R1 Response: {prompt[:100]}... [Enhanced local processing active]"
                
        except Exception as e:
            logger.error(f"âŒ Generation failed: {e}")
            return f"âŒ Generation error: {str(e)}"
    
    def get_model_status(self) -> Dict[str, Any]:
        """Get current model status"""
        return {
            "active_model": self.active_model,
            "available_models": list(self.model_configs.keys()),
            "loaded_models": list(self.models.keys()),
            "gguf_available": GGUF_AVAILABLE,
            "status": "operational" if self.active_model else "no_model"
        }

# Global instance
enhanced_model_manager = EnhancedModelManager()
