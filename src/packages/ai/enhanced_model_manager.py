#!/usr/bin/env python3
"""
Enhanced AI Model Manager for reVoAgent - GGUF Integration Update
Cost-optimized AI model management with true local GGUF processing

This module implements an enhanced AI model management system featuring:
- DeepSeek R1 GGUF (primary local model with real inference)
- Llama GGUF (secondary local model)
- OpenAI (fallback cloud model)
- Anthropic (fallback cloud model)
- True local processing with zero cost
- Performance monitoring and health checks
- Automatic failover and load balancing
"""

import asyncio
import aiohttp
import json
import time
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import subprocess
import psutil
import os
from pathlib import Path

# Import GGUF Manager
try:
    from .gguf_model_manager import GGUFModelManager, GGUFResponse, GGUF_AVAILABLE
    GGUF_INTEGRATION = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ GGUF Model Manager integration available")
except ImportError:
    GGUF_INTEGRATION = False
    GGUF_AVAILABLE = False
    GGUFModelManager = None
    GGUFResponse = None
    logger = logging.getLogger(__name__)
    logger.warning("‚ö†Ô∏è GGUF Model Manager integration not available")

# Configure logging
logging.basicConfig(level=logging.INFO)

class ModelType(Enum):
    """AI model types"""
    LOCAL_OPENSOURCE = "local_opensource"  # DeepSeek R1
    LOCAL_COMMERCIAL = "local_commercial"  # Llama
    CLOUD_OPENAI = "cloud_openai"         # OpenAI
    CLOUD_ANTHROPIC = "cloud_anthropic"   # Anthropic

class ModelStatus(Enum):
    """Model availability status"""
    AVAILABLE = "available"
    UNAVAILABLE = "unavailable"
    LOADING = "loading"
    ERROR = "error"
    MAINTENANCE = "maintenance"

@dataclass
class ModelConfig:
    """AI model configuration"""
    model_id: str
    name: str
    model_type: ModelType
    priority: int  # Lower number = higher priority
    cost_per_token: float  # Cost in USD per 1000 tokens
    max_tokens: int
    context_length: int
    local_path: Optional[str] = None
    api_endpoint: Optional[str] = None
    api_key: Optional[str] = None
    health_check_url: Optional[str] = None
    memory_requirement: Optional[int] = None  # MB
    gpu_requirement: bool = False
    status: ModelStatus = ModelStatus.UNAVAILABLE
    last_health_check: Optional[datetime] = None
    error_count: int = 0
    success_count: int = 0
    average_response_time: float = 0.0
    gguf_enabled: bool = False  # New field for GGUF support

@dataclass
class GenerationRequest:
    """AI generation request"""
    prompt: str
    model_preference: Optional[str] = "auto"
    max_tokens: int = 1000
    temperature: float = 0.7
    force_local: bool = True
    force_cloud: bool = False
    fallback_allowed: bool = True
    cost_limit: Optional[float] = None  # Maximum cost in USD

@dataclass
class GenerationResponse:
    """AI generation response"""
    content: str
    model_used: str
    model_type: ModelType
    tokens_used: int
    cost: float
    response_time: float
    success: bool
    error_message: Optional[str] = None
    fallback_used: bool = False
    gguf_response: bool = False  # New field to indicate GGUF usage

class EnhancedModelManager:
    """
    Enhanced AI Model Manager with GGUF Integration
    
    Provides intelligent model selection with cost optimization,
    automatic failover, and true local GGUF processing.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize the enhanced model manager"""
        self.config = config or {}
        
        # Model configurations
        self.models: Dict[str, ModelConfig] = {}
        
        # GGUF Model Manager for local inference
        self.gguf_manager = None
        if GGUF_INTEGRATION:
            self.gguf_manager = GGUFModelManager(config)
        
        # Performance metrics
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_cost": 0.0,
            "cost_saved": 0.0,  # Savings from using local models
            "local_usage_percentage": 0.0,
            "average_response_time": 0.0,
            "model_usage": {},
            "gguf_requests": 0,  # New metric for GGUF usage
            "gguf_cost_savings": 0.0  # Cost savings from GGUF
        }
        
        # Performance metrics alias for compatibility
        self.performance_metrics = self.metrics
        
        # Health monitoring
        self.health_check_interval = 300  # 5 minutes
        self.health_check_task = None
        
        # Initialize models
        self._initialize_models()
        
        logger.info("ü§ñ Enhanced AI Model Manager initialized with GGUF integration")
        logger.info(f"üìä Configured {len(self.models)} AI models")
        if GGUF_INTEGRATION:
            logger.info("‚úÖ GGUF local inference enabled")
        else:
            logger.warning("‚ö†Ô∏è GGUF local inference not available")

    def _initialize_models(self):
        """Initialize AI model configurations with GGUF support"""
        
        # 1. DeepSeek R1 0528 (Primary - Local/Opensource with GGUF)
        deepseek_model = ModelConfig(
            model_id="deepseek-r1",
            name="DeepSeek R1 0528",
            model_type=ModelType.LOCAL_OPENSOURCE,
            priority=1,  # Highest priority
            cost_per_token=0.0,  # Free local model
            max_tokens=4000,
            context_length=32768,
            local_path=self.config.get("deepseek_path", "models/deepseek-r1"),
            health_check_url="http://localhost:8001/health",
            memory_requirement=8192,  # 8GB RAM
            gpu_requirement=False,  # GGUF can run on CPU
            gguf_enabled=True  # Enable GGUF support
        )
        
        # Check if GGUF model is available
        if GGUF_INTEGRATION and self.gguf_manager:
            if "deepseek-r1" in self.gguf_manager.model_configs:
                deepseek_model.status = ModelStatus.AVAILABLE
                logger.info("‚úÖ DeepSeek R1 GGUF model configuration found")
            else:
                deepseek_model.status = ModelStatus.UNAVAILABLE
                logger.warning("‚ö†Ô∏è DeepSeek R1 GGUF model configuration not found")
        else:
            deepseek_model.status = ModelStatus.UNAVAILABLE
            logger.warning("‚ö†Ô∏è GGUF integration not available for DeepSeek R1")
        
        self.models["deepseek-r1"] = deepseek_model
        
        # 2. Llama (Secondary - Local with potential GGUF support)
        self.models["llama"] = ModelConfig(
            model_id="llama",
            name="Llama 3.1 70B",
            model_type=ModelType.LOCAL_COMMERCIAL,
            priority=2,
            cost_per_token=0.0,  # Free local model
            max_tokens=4000,
            context_length=8192,
            local_path=self.config.get("llama_path", "models/llama-3.1-70b"),
            health_check_url="http://localhost:8002/health",
            memory_requirement=16384,  # 16GB RAM
            gpu_requirement=True,
            gguf_enabled=False  # Can be enabled when GGUF model is available
        )
        
        # 3. OpenAI (Fallback - Cloud)
        self.models["openai-gpt4"] = ModelConfig(
            model_id="openai-gpt4",
            name="OpenAI GPT-4",
            model_type=ModelType.CLOUD_OPENAI,
            priority=3,
            cost_per_token=0.03,  # $30 per 1M tokens
            max_tokens=4000,
            context_length=8192,
            api_endpoint="https://api.openai.com/v1/chat/completions",
            api_key=self.config.get("openai_api_key"),
            health_check_url="https://api.openai.com/v1/models"
        )
        
        # 4. Anthropic (Fallback - Cloud)
        self.models["anthropic-claude"] = ModelConfig(
            model_id="anthropic-claude",
            name="Anthropic Claude 3.5 Sonnet",
            model_type=ModelType.CLOUD_ANTHROPIC,
            priority=4,
            cost_per_token=0.015,  # $15 per 1M tokens
            max_tokens=4000,
            context_length=200000,
            api_endpoint="https://api.anthropic.com/v1/messages",
            api_key=self.config.get("anthropic_api_key"),
            health_check_url="https://api.anthropic.com/v1/messages"
        )
    
    async def initialize_gguf_models(self):
        """Initialize GGUF models for local processing"""
        if not GGUF_INTEGRATION or not self.gguf_manager:
            logger.warning("‚ö†Ô∏è GGUF integration not available")
            return False
        
        logger.info("üîÑ Initializing GGUF models...")
        
        # Load DeepSeek R1 GGUF model
        if "deepseek-r1" in self.gguf_manager.model_configs:
            success = await self.gguf_manager.load_model("deepseek-r1")
            if success:
                self.models["deepseek-r1"].status = ModelStatus.AVAILABLE
                logger.info("‚úÖ DeepSeek R1 GGUF model loaded successfully")
                return True
            else:
                self.models["deepseek-r1"].status = ModelStatus.ERROR
                logger.error("‚ùå Failed to load DeepSeek R1 GGUF model")
                return False
        
        return False
    
    def start_health_monitoring(self):
        """Start health monitoring (call this in an async context)"""
        try:
            if self.health_check_task is None:
                self.health_check_task = asyncio.create_task(self._health_monitor())
                logger.info("üîç Health monitoring started")
        except RuntimeError:
            # No event loop running, health monitoring will be started later
            logger.info("‚è≥ Health monitoring will start when event loop is available")
    
    def stop_health_monitoring(self):
        """Stop health monitoring"""
        if self.health_check_task:
            self.health_check_task.cancel()
            self.health_check_task = None
            logger.info("üõë Health monitoring stopped")

    async def _health_monitor(self):
        """Monitor model health and availability"""
        while True:
            try:
                for model_id, model in self.models.items():
                    await self._check_model_health(model)
                
                await asyncio.sleep(self.health_check_interval)
                
            except Exception as e:
                logger.error(f"Error in health monitor: {e}")
                await asyncio.sleep(60)

    async def _check_model_health(self, model: ModelConfig):
        """Check individual model health"""
        try:
            if model.gguf_enabled and GGUF_INTEGRATION and self.gguf_manager:
                # Check GGUF model health
                await self._check_gguf_model_health(model)
            elif model.model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]:
                # Check traditional local model health
                await self._check_local_model_health(model)
            else:
                # Check cloud model health
                await self._check_cloud_model_health(model)
                
            model.last_health_check = datetime.now(timezone.utc)
            
        except Exception as e:
            logger.error(f"Health check failed for {model.name}: {e}")
            model.status = ModelStatus.ERROR
            model.error_count += 1

    async def _check_gguf_model_health(self, model: ModelConfig):
        """Check GGUF model health"""
        if self.gguf_manager and self.gguf_manager.is_model_loaded(model.model_id):
            model.status = ModelStatus.AVAILABLE
        else:
            model.status = ModelStatus.UNAVAILABLE

    async def _check_local_model_health(self, model: ModelConfig):
        """Check local model health"""
        # Check if model files exist
        if model.local_path and not os.path.exists(model.local_path):
            model.status = ModelStatus.UNAVAILABLE
            return
        
        # Check system resources
        memory = psutil.virtual_memory()
        available_memory = memory.available // (1024 * 1024)  # MB
        
        if model.memory_requirement and available_memory < model.memory_requirement:
            model.status = ModelStatus.UNAVAILABLE
            logger.warning(f"Insufficient memory for {model.name}: {available_memory}MB < {model.memory_requirement}MB")
            return
        
        # Check GPU availability if required
        if model.gpu_requirement:
            try:
                import torch
                if not torch.cuda.is_available():
                    model.status = ModelStatus.UNAVAILABLE
                    logger.warning(f"GPU required but not available for {model.name}")
                    return
            except ImportError:
                logger.warning("PyTorch not available for GPU check")
        
        # Check health endpoint if available
        if model.health_check_url:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(model.health_check_url, timeout=5) as response:
                        if response.status == 200:
                            model.status = ModelStatus.AVAILABLE
                        else:
                            model.status = ModelStatus.UNAVAILABLE
            except:
                model.status = ModelStatus.UNAVAILABLE
        else:
            # Assume available if no health check URL
            model.status = ModelStatus.AVAILABLE

    async def _check_cloud_model_health(self, model: ModelConfig):
        """Check cloud model health"""
        if not model.api_key:
            model.status = ModelStatus.UNAVAILABLE
            return
        
        try:
            headers = {}
            if model.model_type == ModelType.CLOUD_OPENAI:
                headers["Authorization"] = f"Bearer {model.api_key}"
            elif model.model_type == ModelType.CLOUD_ANTHROPIC:
                headers["x-api-key"] = model.api_key
                headers["anthropic-version"] = "2023-06-01"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(model.health_check_url, headers=headers, timeout=10) as response:
                    if response.status in [200, 401]:  # 401 means API is working but key might be invalid
                        model.status = ModelStatus.AVAILABLE
                    else:
                        model.status = ModelStatus.UNAVAILABLE
                        
        except Exception as e:
            logger.error(f"Cloud health check failed for {model.name}: {e}")
            model.status = ModelStatus.UNAVAILABLE

    async def generate_response(self, request: GenerationRequest) -> GenerationResponse:
        """Generate AI response with intelligent model selection and GGUF integration"""
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        # Select best available model
        selected_model = await self._select_model(request)
        
        if not selected_model:
            self.metrics["failed_requests"] += 1
            return GenerationResponse(
                content="",
                model_used="none",
                model_type=ModelType.LOCAL_OPENSOURCE,
                tokens_used=0,
                cost=0.0,
                response_time=time.time() - start_time,
                success=False,
                error_message="No available models"
            )
        
        # Generate response with selected model
        try:
            response = await self._generate_with_model(selected_model, request)
            response.response_time = time.time() - start_time
            
            # Update metrics
            self.metrics["successful_requests"] += 1
            self.metrics["total_cost"] += response.cost
            
            # Track GGUF usage
            if response.gguf_response:
                self.metrics["gguf_requests"] += 1
                # Calculate estimated cloud cost for comparison
                estimated_cloud_cost = response.tokens_used * 0.03 / 1000
                self.metrics["gguf_cost_savings"] += estimated_cloud_cost
            
            # Calculate cost savings
            if selected_model.model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]:
                # Estimate what it would have cost with cloud models
                cloud_cost = response.tokens_used * 0.03 / 1000  # Assume GPT-4 pricing
                self.metrics["cost_saved"] += cloud_cost
            
            # Update model metrics
            selected_model.success_count += 1
            selected_model.average_response_time = (
                (selected_model.average_response_time * (selected_model.success_count - 1) + response.response_time) /
                selected_model.success_count
            )
            
            # Update usage statistics
            model_id = selected_model.model_id
            if model_id not in self.metrics["model_usage"]:
                self.metrics["model_usage"][model_id] = 0
            self.metrics["model_usage"][model_id] += 1
            
            # Calculate local usage percentage
            local_usage = sum(
                count for model_id, count in self.metrics["model_usage"].items()
                if self.models[model_id].model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]
            )
            self.metrics["local_usage_percentage"] = (local_usage / self.metrics["total_requests"]) * 100
            
            gguf_indicator = " (GGUF)" if response.gguf_response else ""
            logger.info(f"‚úÖ Generated response using {selected_model.name}{gguf_indicator} (cost: ${response.cost:.4f})")
            return response
            
        except Exception as e:
            self.metrics["failed_requests"] += 1
            selected_model.error_count += 1
            
            # Try fallback if allowed
            if request.fallback_allowed and selected_model.priority < 4:
                logger.warning(f"Model {selected_model.name} failed, trying fallback...")
                fallback_request = GenerationRequest(
                    prompt=request.prompt,
                    model_preference="auto",
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                    force_local=False,  # Allow cloud fallback
                    fallback_allowed=False  # Prevent infinite recursion
                )
                
                fallback_response = await self.generate_response(fallback_request)
                fallback_response.fallback_used = True
                return fallback_response
            
            return GenerationResponse(
                content="",
                model_used=selected_model.model_id,
                model_type=selected_model.model_type,
                tokens_used=0,
                cost=0.0,
                response_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )

    async def _select_model(self, request: GenerationRequest) -> Optional[ModelConfig]:
        """Select the best available model based on request preferences"""
        
        # If specific model requested
        if request.model_preference and request.model_preference != "auto":
            if request.model_preference in self.models:
                model = self.models[request.model_preference]
                if model.status == ModelStatus.AVAILABLE:
                    return model
        
        # Auto-selection based on priority and constraints
        available_models = [
            model for model in self.models.values()
            if model.status == ModelStatus.AVAILABLE
        ]
        
        # Filter by local preference
        if request.force_local:
            local_models = [
                model for model in available_models
                if model.model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]
            ]
            if local_models:
                available_models = local_models
        
        # Filter by cost limit
        if request.cost_limit:
            cost_filtered = [
                model for model in available_models
                if model.cost_per_token * request.max_tokens / 1000 <= request.cost_limit
            ]
            if cost_filtered:
                available_models = cost_filtered
        
        # Sort by priority (lower number = higher priority)
        available_models.sort(key=lambda m: m.priority)
        
        return available_models[0] if available_models else None

    async def _generate_with_model(self, model: ModelConfig, request: GenerationRequest) -> GenerationResponse:
        """Generate response with specific model"""
        
        # Check if this is a GGUF-enabled local model
        if (model.gguf_enabled and GGUF_INTEGRATION and self.gguf_manager and 
            self.gguf_manager.is_model_loaded(model.model_id)):
            return await self._generate_gguf(model, request)
        elif model.model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]:
            return await self._generate_local(model, request)
        elif model.model_type == ModelType.CLOUD_OPENAI:
            return await self._generate_openai(model, request)
        elif model.model_type == ModelType.CLOUD_ANTHROPIC:
            return await self._generate_anthropic(model, request)
        else:
            raise ValueError(f"Unsupported model type: {model.model_type}")

    async def _generate_gguf(self, model: ModelConfig, request: GenerationRequest) -> GenerationResponse:
        """Generate response using GGUF model manager"""
        
        logger.info(f"üîÑ Using GGUF inference for {model.name}")
        
        # Generate response using GGUF manager
        gguf_response = await self.gguf_manager.generate_response(
            model.model_id,
            request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        if gguf_response.success:
            return GenerationResponse(
                content=gguf_response.text,
                model_used=model.model_id,
                model_type=model.model_type,
                tokens_used=gguf_response.tokens_generated,
                cost=0.0,  # GGUF models are always free
                response_time=gguf_response.inference_time,
                success=True,
                gguf_response=True  # Mark as GGUF response
            )
        else:
            return GenerationResponse(
                content="",
                model_used=model.model_id,
                model_type=model.model_type,
                tokens_used=0,
                cost=0.0,
                response_time=gguf_response.inference_time,
                success=False,
                error_message=gguf_response.error_message,
                gguf_response=True
            )

    async def _generate_local(self, model: ModelConfig, request: GenerationRequest) -> GenerationResponse:
        """Generate response with traditional local model (fallback simulation)"""
        
        # For demo purposes, simulate local model response
        # In production, this would interface with actual local model APIs
        
        await asyncio.sleep(0.5)  # Simulate processing time
        
        # Simulate response based on model
        if model.model_id == "deepseek-r1":
            content = f"[DeepSeek R1 Simulated] {request.prompt[:50]}... (Simulated local response - GGUF not available)"
        elif model.model_id == "llama":
            content = f"[Llama Simulated] {request.prompt[:50]}... (Simulated local response)"
        else:
            content = f"[Local Model Simulated] {request.prompt[:50]}..."
        
        tokens_used = min(len(content.split()) * 1.3, request.max_tokens)  # Rough token estimate
        
        return GenerationResponse(
            content=content,
            model_used=model.model_id,
            model_type=model.model_type,
            tokens_used=int(tokens_used),
            cost=0.0,  # Local models are free
            response_time=0.0,  # Will be set by caller
            success=True
        )

    async def _generate_openai(self, model: ModelConfig, request: GenerationRequest) -> GenerationResponse:
        """Generate response with OpenAI model"""
        
        headers = {
            "Authorization": f"Bearer {model.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4",
            "messages": [{"role": "user", "content": request.prompt}],
            "max_tokens": request.max_tokens,
            "temperature": request.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(model.api_endpoint, headers=headers, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data["choices"][0]["message"]["content"]
                    tokens_used = data["usage"]["total_tokens"]
                    cost = tokens_used * model.cost_per_token / 1000
                    
                    return GenerationResponse(
                        content=content,
                        model_used=model.model_id,
                        model_type=model.model_type,
                        tokens_used=tokens_used,
                        cost=cost,
                        response_time=0.0,
                        success=True
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"OpenAI API error: {response.status} - {error_text}")

    async def _generate_anthropic(self, model: ModelConfig, request: GenerationRequest) -> GenerationResponse:
        """Generate response with Anthropic model"""
        
        headers = {
            "x-api-key": model.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "messages": [{"role": "user", "content": request.prompt}]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(model.api_endpoint, headers=headers, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data["content"][0]["text"]
                    tokens_used = data["usage"]["input_tokens"] + data["usage"]["output_tokens"]
                    cost = tokens_used * model.cost_per_token / 1000
                    
                    return GenerationResponse(
                        content=content,
                        model_used=model.model_id,
                        model_type=model.model_type,
                        tokens_used=tokens_used,
                        cost=cost,
                        response_time=0.0,
                        success=True
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"Anthropic API error: {response.status} - {error_text}")

    async def list_available_models(self) -> List[Dict[str, Any]]:
        """List all available models with their status"""
        models = []
        
        for model in self.models.values():
            model_info = {
                "model_id": model.model_id,
                "name": model.name,
                "type": model.model_type.value,
                "priority": model.priority,
                "status": model.status.value,
                "cost_per_token": model.cost_per_token,
                "max_tokens": model.max_tokens,
                "context_length": model.context_length,
                "success_rate": (
                    model.success_count / max(model.success_count + model.error_count, 1)
                ) * 100,
                "average_response_time": model.average_response_time,
                "gguf_enabled": model.gguf_enabled
            }
            
            # Add GGUF-specific information
            if model.gguf_enabled and GGUF_INTEGRATION and self.gguf_manager:
                gguf_info = self.gguf_manager.get_model_info(model.model_id)
                if gguf_info:
                    model_info["gguf_loaded"] = gguf_info["loaded"]
                    model_info["gguf_memory_usage"] = gguf_info["memory_usage_mb"]
                    model_info["gguf_inferences"] = gguf_info["inferences"]
            
            models.append(model_info)
        
        return sorted(models, key=lambda m: m["priority"])
    
    def get_available_providers(self) -> List[str]:
        """Get list of available model providers (sync method for compatibility)"""
        return [model.model_id for model in self.models.values() if model.status == ModelStatus.AVAILABLE]
    
    def get_cost_statistics(self) -> Dict[str, Any]:
        """Get cost optimization statistics with GGUF metrics"""
        total_requests = self.metrics["successful_requests"] + self.metrics["failed_requests"]
        local_requests = sum(
            model.success_count for model in self.models.values()
            if model.model_type in [ModelType.LOCAL_OPENSOURCE, ModelType.LOCAL_COMMERCIAL]
        )
        
        local_percentage = (local_requests / max(total_requests, 1)) * 100
        gguf_percentage = (self.metrics["gguf_requests"] / max(total_requests, 1)) * 100
        
        return {
            "total_requests": total_requests,
            "local_requests": local_requests,
            "gguf_requests": self.metrics["gguf_requests"],
            "local_percentage": local_percentage,
            "gguf_percentage": gguf_percentage,
            "total_cost": self.metrics["total_cost"],
            "cost_savings": max(0, (total_requests * 0.03) - self.metrics["total_cost"]),
            "gguf_cost_savings": self.metrics["gguf_cost_savings"],
            "average_cost_per_request": self.metrics["total_cost"] / max(total_requests, 1)
        }
    
    def generate_response_sync(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Generate response (sync method for compatibility)"""
        # For compatibility, return a simple response without async processing
        # In production, this would run the async version in an event loop
        
        # Simulate model selection and response
        available_models = [model for model in self.models.values()]
        if available_models:
            # Use highest priority model
            model = min(available_models, key=lambda m: m.priority)
            provider = model.model_id
            
            # Check if GGUF is available for this model
            is_gguf = model.gguf_enabled and GGUF_INTEGRATION
            cost = 0.0
            quality_score = 0.90 if is_gguf else 0.85
            
        else:
            provider = "fallback"
            cost = 0.0
            quality_score = 0.80
            is_gguf = False
        
        gguf_indicator = " (GGUF)" if is_gguf else ""
        
        return {
            "content": f"[{provider.upper()}{gguf_indicator}] {prompt[:50]}... (Cost-optimized response)",
            "provider": provider,
            "cost": cost,
            "quality_score": quality_score,
            "tokens_used": len(prompt.split()) * 2,  # Rough estimate
            "success": True,
            "gguf_enabled": is_gguf
        }

    async def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive model manager metrics including GGUF stats"""
        
        # Calculate average response time
        if self.metrics["successful_requests"] > 0:
            total_time = sum(
                model.average_response_time * model.success_count
                for model in self.models.values()
            )
            self.metrics["average_response_time"] = total_time / self.metrics["successful_requests"]
        
        # Get GGUF metrics if available
        gguf_metrics = {}
        if GGUF_INTEGRATION and self.gguf_manager:
            gguf_metrics = self.gguf_manager.get_metrics()
        
        return {
            "requests": {
                "total": self.metrics["total_requests"],
                "successful": self.metrics["successful_requests"],
                "failed": self.metrics["failed_requests"],
                "gguf_requests": self.metrics["gguf_requests"],
                "success_rate": (
                    self.metrics["successful_requests"] / max(self.metrics["total_requests"], 1)
                ) * 100
            },
            "cost_optimization": {
                "total_cost": self.metrics["total_cost"],
                "cost_saved": self.metrics["cost_saved"],
                "gguf_cost_savings": self.metrics["gguf_cost_savings"],
                "local_usage_percentage": self.metrics["local_usage_percentage"],
                "savings_rate": (
                    self.metrics["cost_saved"] / max(self.metrics["cost_saved"] + self.metrics["total_cost"], 1)
                ) * 100
            },
            "performance": {
                "average_response_time": self.metrics["average_response_time"]
            },
            "model_usage": self.metrics["model_usage"],
            "model_health": {
                model_id: {
                    "status": model.status.value,
                    "success_count": model.success_count,
                    "error_count": model.error_count,
                    "gguf_enabled": model.gguf_enabled,
                    "last_health_check": model.last_health_check.isoformat() if model.last_health_check else None
                }
                for model_id, model in self.models.items()
            },
            "gguf_metrics": gguf_metrics
        }

    def get_health_status(self) -> Dict[str, Any]:
        """Get health status for compatibility"""
        return {
            "status": "healthy",
            "models_available": len([m for m in self.models.values() if m.status == ModelStatus.AVAILABLE]),
            "total_models": len(self.models),
            "gguf_integration": GGUF_INTEGRATION,
            "gguf_models_loaded": len([m for m in self.models.values() if m.gguf_enabled and m.status == ModelStatus.AVAILABLE])
        }

    async def shutdown(self):
        """Shutdown the model manager"""
        logger.info("üõë Shutting down Enhanced Model Manager...")
        
        if self.health_check_task:
            self.health_check_task.cancel()
        
        # Shutdown GGUF manager if available
        if GGUF_INTEGRATION and self.gguf_manager:
            self.gguf_manager.unload_all_models()
        
        logger.info("üõë Enhanced Model Manager shutdown complete")

# Example usage and testing
async def main():
    """Example usage of Enhanced Model Manager with GGUF integration"""
    
    print("ü§ñ Enhanced AI Model Manager with GGUF Integration Demo")
    print("=" * 60)
    
    # Configuration with API keys (use environment variables in production)
    config = {
        "deepseek_path": "models/deepseek-r1",
        "llama_path": "models/llama-3.1-70b",
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY")
    }
    
    # Initialize model manager
    manager = EnhancedModelManager(config)
    
    # Initialize GGUF models
    await manager.initialize_gguf_models()
    
    print("‚úÖ Enhanced Model Manager initialized with GGUF integration")
    
    # List available models
    models = await manager.list_available_models()
    print(f"üìã Available models:")
    for model in models:
        gguf_status = " (GGUF loaded)" if model.get("gguf_loaded", False) else ""
        print(f"   - {model['name']} ({model['model_id']}) - {model['status']}{gguf_status}")
    
    # Test generation request
    request = GenerationRequest(
        prompt="Explain the benefits of local AI models for cost optimization",
        model_preference="auto",
        max_tokens=500,
        temperature=0.7,
        force_local=True
    )
    
    print(f"\nüîÑ Generating response...")
    response = await manager.generate_response(request)
    
    print(f"‚úÖ Response generated:")
    print(f"   - Model used: {response.model_used}")
    print(f"   - Model type: {response.model_type.value}")
    print(f"   - GGUF response: {response.gguf_response}")
    print(f"   - Tokens: {response.tokens_used}")
    print(f"   - Cost: ${response.cost:.4f}")
    print(f"   - Response time: {response.response_time:.2f}s")
    print(f"   - Success: {response.success}")
    
    # Get metrics
    metrics = await manager.get_metrics()
    print(f"\nüìä Model Manager Metrics:")
    print(f"   - Total requests: {metrics['requests']['total']}")
    print(f"   - GGUF requests: {metrics['requests']['gguf_requests']}")
    print(f"   - Success rate: {metrics['requests']['success_rate']:.1f}%")
    print(f"   - Local usage: {metrics['cost_optimization']['local_usage_percentage']:.1f}%")
    print(f"   - Cost saved: ${metrics['cost_optimization']['cost_saved']:.4f}")
    print(f"   - GGUF savings: ${metrics['cost_optimization']['gguf_cost_savings']:.4f}")
    
    await manager.shutdown()

if __name__ == "__main__":
    asyncio.run(main())
